[[moc-clippings]]

# Co-Intelligence: Living and Working With AI

**Page 10**

> The latest AI boom started in the 2010s with the promise of using machine learning techniques for data analysis and prediction. Many of these applications used a technique called supervised learning, which means these forms of AI needed labeled data to learn from. Labeled data is data that has been annotated with the correct answers or outputs for a given task. For example, if you want to train an AI system to recognize faces, you need to provide it with images of faces that have been labeled with the names or identities of the people in them. This phase of AI was the domain of larger organizations that had vast amounts of data. They used these tools as powerful prediction systems, whether optimizing shipping logistics or guessing what kind of content to show you based on your browsing history. You might have heard the buzzwords big data or algorithmic decision-making describing these kinds of uses. Consumers mostly saw the benefits of machine learning when these techniques were integrated into tools such as voice recognition systems or translation apps.

**Page 12**

> But among the many papers on different forms of AI being published by industry and academic experts, one stood out, a paper with the catchy title “Attention Is All You Need.” Published by Google researchers in 2017, this paper introduced a significant shift in the world of AI, particularly in how computers understand and process human language. This paper proposed a new architecture, called the Transformer, that could be used to help a computer better process how humans communicate. Before the Transformer, other methods were used to teach computers to understand language, but they had limitations that severely curtailed their usefulness. The Transformer solved these issues by utilizing an “attention mechanism.” This technique allows the AI to concentrate on the most relevant parts of a text, making it easier for the AI to understand and work with language in a way that seemed more human.

**Page 14**

> To teach AI how to understand and generate humanlike writing, it is trained on a massive amount of text from various sources, such as websites, books, and other digital documents. This is called pretraining, and unlike earlier forms of AI, it is unsupervised, which means the AI doesn’t need carefully labeled data. Instead, by analyzing these examples, AI learns to recognize patterns, structures, and context in human language. Remarkably, with a vast number of adjustable parameters (called weights), LLMs can create a model that emulates how humans communicate through written text. Weights are complex mathematical transformations that LLMs learn from reading those billions of words, and they tell the AI how likely different words or parts of words are to appear together or in a certain order. The original ChatGPT had 175 billion weights, encoding the connection between words and parts of words. No one programmed these weights; instead, they are learned by the AI itself during its training.

**Page 14**

> results each time you ask them a question. To

**Page 15**

> Many of the AI companies keep the source text they train from, called training corpuses, secret, but a typical example of training data largely consists of text pulled from the internet, public domain books and research articles, and assorted other free sources of content that researchers can find. Actually looking into these sources in detail reveals some odd material. For example, the entire email database of Enron, shut down for corporate fraud, is used as part of the training material for many AIs, simply because it was made freely available to AI researchers. Similarly, there is a tremendous amount of amateur romance novels included in training data, as the internet is full of amateur novelists. The search for high-quality content for training material has become a major topic in AI development, since information-hungry AI companies are running out of good, free sources. As a result, it is also likely that most AI training data contains copyrighted information, like books used without permission, whether by accident or on purpose. The legal implications of this are still unclear. Since the data is used to create weights, and not directly copied into the AI systems, some experts consider it to be outside standard copyright law.

**Page 16**

> Because of the variety of data sources used, learning is not always a good thing. AI can also learn biases, errors, and falsehoods from the data it sees. Just out of pretraining, the AI also doesn’t necessarily produce the sorts of outcomes that people would expect in response to a prompt. And, potentially worse, it has no ethical boundaries and would be happy to give advice on how to embezzle money, commit murder, or stalk someone online. LLMs in this pretrained mode just reflect back whatever they were trained on, like a mirror, without applying any judgment. So, after learning from all the text examples in pretraining, many LLMs undergo further improvement in a second stage, called fine-tuning.

**Page 16**

> (one estimate suggests that high-quality data, like online books and academic articles, will be exhausted by 2026),

**Page 16**

> There is also active research into understanding whether AI can pretrain on its own content. This is what chess-playing AIs already do, learning by playing games against themselves, but it is not yet clear whether it will work for LLMs.

**Page 17**

> One important fine-tuning approach is to bring humans into the process, which had previously been mostly automated. AI companies hire workers, some highly paid experts, others low-paid contract workers in English-speaking nations like Kenya, to read AI answers and judge them on various characteristics. In some cases, that might be rating results for accuracy, in others it might be to screen out violent or pornographic answers. That feedback is then used to do additional training, fine-tuning the AI’s performance to fit the preferences of the human, providing additional learning that reinforces good answers and reduces bad answers, which is why the process is called Reinforcement Learning from Human Feedback (RLHF).

**Page 5**

> General Purpose Technologies typically have slow adoption, as they require many other technologies to work well. The internet is a great example. While it was born as ARPANET in the late 1960s, it took nearly three decades to achieve general use in the 1990s, with the invention of the web browser, the development of affordable computers, and the growing infrastructure to support high-speed internet. It was fifty years before smartphones enabled the rise of social media. And many companies have not even fully embraced the internet: making a business “digital” is still a hot topic of discussion at business school, especially as many banks still use mainframe computers.

**Page 5**

> generative AI might even be bigger. General

**Page 6**

> Early computers improved quickly, thanks to Moore’s Law, the long-standing trend that the capability of computers doubles every two years.

**Page 6**

> ChatGPT reached 100 million users faster than any previous product in history, driven by the fact that it was free to access, available to individuals, and incredibly useful.

