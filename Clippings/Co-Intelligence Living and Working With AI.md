[[moc-clippings]]

# Co-Intelligence: Living and Working With AI

**Page 15**

> Many of the AI companies keep the source text they train from, called training corpuses, secret, but a typical example of training data largely consists of text pulled from the internet, public domain books and research articles, and assorted other free sources of content that researchers can find. Actually looking into these sources in detail reveals some odd material. For example, the entire email database of Enron, shut down for corporate fraud, is used as part of the training material for many AIs, simply because it was made freely available to AI researchers. Similarly, there is a tremendous amount of amateur romance novels included in training data, as the internet is full of amateur novelists. The search for high-quality content for training material has become a major topic in AI development, since information-hungry AI companies are running out of good, free sources. As a result, it is also likely that most AI training data contains copyrighted information, like books used without permission, whether by accident or on purpose. The legal implications of this are still unclear. Since the data is used to create weights, and not directly copied into the AI systems, some experts consider it to be outside standard copyright law.

**Page 16**

> (one estimate suggests that high-quality data, like online books and academic articles, will be exhausted by 2026),

**Page 16**

> Because of the variety of data sources used, learning is not always a good thing. AI can also learn biases, errors, and falsehoods from the data it sees. Just out of pretraining, the AI also doesn’t necessarily produce the sorts of outcomes that people would expect in response to a prompt. And, potentially worse, it has no ethical boundaries and would be happy to give advice on how to embezzle money, commit murder, or stalk someone online. LLMs in this pretrained mode just reflect back whatever they were trained on, like a mirror, without applying any judgment. So, after learning from all the text examples in pretraining, many LLMs undergo further improvement in a second stage, called fine-tuning. One important fine-tuning approach is to bring humans into the process, which had previously been mostly automated. AI companies hire workers, some highly paid experts, others low-paid contract workers in English-speaking nations like Kenya, to read AI answers and judge them on various characteristics. In some cases, that might be rating results for accuracy, in others it might be to screen out violent or pornographic answers. That feedback is then used to do additional training, fine-tuning the AI’s performance to fit the preferences of the human, providing additional learning that reinforces good answers and reduces bad answers, which is why the process is called Reinforcement Learning from Human Feedback (RLHF).

**Page 16**

> This is what chess-playing AIs already do, learning by playing games against themselves, but it is not yet clear whether it will work for LLMs.

**Page 6**

> ChatGPT reached 100 million users faster than any previous product in history, driven by the fact that it was free to access, available to individuals, and incredibly useful.

**Page 6**

> Early computers improved quickly, thanks to Moore’s Law, the long-standing trend that the capability of computers doubles every two years.
